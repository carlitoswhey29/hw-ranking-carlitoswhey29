\documentclass[12pt]{article}
\usepackage{times} 			% use Times New Roman font

\usepackage[margin=1in]{geometry}   % sets 1 inch margins on all sides
\usepackage{hyperref}               % for URL formatting
\usepackage[pdftex]{graphicx}       % So includegraphics will work
\setlength{\parskip}{1em}           % skip 1em between paragraphs
\usepackage{indentfirst}            % indent the first line of each paragraph
\usepackage{datetime}
\usepackage[small, bf]{caption}
\usepackage{listings}               % for code listings
\usepackage{xcolor}                 % for styling code
\usepackage{multirow}

%New colors defined below
\definecolor{backcolour}{RGB}{246, 246, 246}   % 0xF6, 0xF6, 0xF6
\definecolor{codegreen}{RGB}{16, 124, 2}       % 0x10, 0x7C, 0x02
\definecolor{codepurple}{RGB}{170, 0, 217}     % 0xAA, 0x00, 0xD9
\definecolor{codered}{RGB}{154, 0, 18}         % 0x9A, 0x00, 0x12

%Code listing style named "gcolabstyle" - matches Google Colab
\lstdefinestyle{gcolabstyle}{
  basicstyle=\ttfamily\small,
  backgroundcolor=\color{backcolour},   
  commentstyle=\itshape\color{codegreen},
  keywordstyle=\color{codepurple},
  stringstyle=\color{codered},
  numberstyle=\ttfamily\footnotesize\color{darkgray}, 
  breakatwhitespace=false,         
  breaklines=true,                 
  captionpos=b,                    
  keepspaces=true,                 
  numbers=left,                    
  numbersep=5pt,                  
  showspaces=false,                
  showstringspaces=false,
  showtabs=false,                  
  tabsize=2
}

\lstset{style=gcolabstyle}      %set gcolabstyle code listing

% to make long URIs break nicely
\makeatletter
\g@addto@macro{\UrlBreaks}{\UrlOrds}
\makeatother

% for fancy page headings
\usepackage{fancyhdr}
\setlength{\headheight}{13.6pt} % to remove fancyhdr warning
\pagestyle{fancy}
\fancyhf{}
\rhead{\small \thepage}
\lhead{\small HW 3, AGUILAR}  % EDIT THIS, REPLACE # with HW number
\chead{\small CS 432, Spring 2021} 
%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
\begin{document}

\begin{centering}
{\large\textbf{HW3-report}}\\ % EDIT THIS
                                % REPLACE # with HW num and ADD title
CARLOS AGUILAR\\                     % EDIT THIS
07 MARCH 2021\\                      % EDIT THIS
\end{centering}

%-------------------------------------------------------------------------

% The * after \section just says to not number the sections
\section*{Q1}
\emph{
Download the content of the 1000 unique URIs you gathered in HW2. You'll need to save the content returned from each URI in a uniquely-named file. Then use a tool to remove (most) of the HTML markup for all 1000 HTML documents.}

\subsection*{Answer}

%Importing code from file
\lstinputlisting[language=Python, caption=Python code for downloading the links, label=lst:import]{http-downloader.py}

\lstinputlisting[language=Python, caption=Python code for extracting the text from the html documents, label=lst:import]{text-extractor.py}

\subsection*{Discussion}
The first challenge was downloading all the files in a timely manner. I knew that I wanted to use wget, and that I had to save the documents to a directory. The first issue I ran across was the constant retrying to download, so I had to place a limit of retries with a flag limiting the tries to 2. It was at that point that I realized that not all the links were still available. I lost 61 links in total out of the original 1000.  After downloading the documents, the next challenge was to convert it to text using the boilerpipe3 tool, through the discussions in Piazza, I was able to gather the correct package. After running the code, I quickly came to a halt after some raised unicode errors, it was at that point that I determined that I had to add a try/except block to my script, with the except portion just continuing. The result was a loss of more documents since it wasn't able to download content due to decoding issues. I parsed through the documents and deleted all the files that had no content saved to them, which gave me a total of 915 documents. 
%-  *******  Fill this out *******

%-------------------------------------------------------------------------
%-------------------------------------------------------------------------
\section*{Q2}
\emph{Rank with TF-IDF.  Choose a query term that is not a stop word , not super-general, and not HTML markup (that is found in at least 10 of your documents. Use Bing or Google as the corpus.}

%URI to request: \url{http://www.cs.odu.edu/~mweigle/courses/cs532/ua_echo.php}

\subsection*{Answer}
Table 1. displays the TF-IDF information for the query term "political". I used Google for the DF estimation (55 B) total size of the corpus. 

\begin{table}[]
    \centering
    \begin{tabular}{|c|c|c|l|}
        \hline
        TF-IDF & TF & IDF  & URI \\
        
        \hline
        0.00467 & 0.00099 & 4.17 &  \url{https://www.salon.com/}$^{1}$  \\
        \hline
        0.00266 & 0.00056 & 4.17 &  \url{https://www.palabranahj.org/archive/}$^{2}$   \\
        \hline
        0.00224 & 0.00047 & 4.17 &   \url{https://www.theatlantic.com/politics/archive/2019/09/}$^{3}$    \\
        \hline
        0.00156 & 0.00033 & 4.17 &   \url{https://www.theatlantic.com/magazine/archive/2020/09/}$^{4}$  \\
        \hline
        0.00147 & 0.00031 & 4.17 &   \url{https://www.theatlantic.com/ideas/archive/2021/01/}$^{5}$   \\
        
        \hline
        0.00137 & 0.00029 & 4.17 &   \url{https://www.theatlantic.com/magazine/archive/2021/03/}$^{6}$   \\
        
        \hline
        0.00128 & 0.00027 & 4.17 &  \url{https://aidnography.blogspot.com/2021/02/}$^{7}$   \\
        \hline
        0.00067 & 0.00014 & 4.17 &   \url{https://web.archive.org/web/20061004124649/}$^{8}$   \\
        \hline
        0.00066 & 0.00014 & 4.17 &  \url{https://harpers.org/archive/2016/04/legalize-it-all/}$^{9}$   \\
        \hline
        0.00023 & 0.00005 & 4.17 &  \url{https://crimethinc.com/2021/03/03/}$^{10}$   \\
        \hline
    \end{tabular}
    \caption{TF-IDF for 10 URIs using Google for DF}
    \label{tab:my_label}
\end{table}


\subsection*{Discussion}
I was able to gather the information required by creating a simple script that runs the sub-process 'wc -c' to determine the number of words in the document, then used 'grep -rni "political" ' to output the usage and which files the term was used. I was able to then select 10 documents from this list, and then modified my script to use 'grep -c' to display the amount of times that it was used in a specific document. With that information, I calculated the TF by (words in a document / amount of times term was used in the document), the IDF was calculated by log2( Google corpus (55 B) / Term result (2.09 B)), the TF-IDF was a multiple of the two.

%-  *******  Fill this out *******

%-------------------------------------------------------------------------
%-------------------------------------------------------------------------

\section*{Q3}

\emph{Rank the domains of those 10 URIs from Q2 by their PageRank. Use any of the free PR estimators on the web}

\subsection*{Answer}
To complete this requirement, I utilized \url{http://www.prchecker.info/check_page_rank.php} which gave me the results below. This portion was unable to be automated.



\begin{tabular}{|l|c|}

    \hline
    URI & PageRank \\
    \hline
    \url{https://www.theatlantic.com/}$^{3-6}$ & 0.8 \\
    
    \hline
    \url{https://web.archive.org/}$^{8}$ & 0.7 \\
    
    \hline
    \url{https://harpers.org/}$^{9}$ & 0.6 \\
    
    \hline
    \url{https://crimethinc.com/}$^{10}$ & 0 \\
    
    \hline
    \url{https://www.palabranahj.org/}$^{2}$ & 0 \\
    
    \hline
    \url{https://www.salon.com/}$^{1}$ & 0 \\
    
    \hline
    \url{https://aidnography.blogspot.com/}$^{7}$ & 0 \\
    \hline
\end{tabular}
\caption{PageRank 10 URIs using \url{http://www.prchecker.info/check_page_rank.php}}

\subsection*{Discussion}
The results of 4 of the URIs resulted in 0, which at a glance wasn't that surprising since I have never heard of them before. Their TF-IDFs were both high and low, so there weren't any consistencies there to conclude that one correlated with the other. TheAtlantic.com soared with a rank of 8 out of 10, and stayed in the middle of my TF-IDF.

\section*{Q4}
\emph{Re-do Q2 using the documents you collected in Q1 as the corpus instead of all of the Web. You should have 1000 total documents collected from Twitter, and you can use grep to discover how many of those documents contain your query term. Compare this ranking to those obtained in Q2 and discuss.}

%URI to request: \url{http://www.cs.odu.edu/~mweigle/courses/cs532/ua_echo.php}

\subsection*{Answer}
Table 1. displays the TF-IDF information for the query term "political". I used the total links for the DF estimation (1000) total size of the corpus. 

\begin{table}[]
    \centering
    \begin{tabular}{|c|c|c|l|}
        \hline
        TF-IDF & TF & IDF  & URI \\
        
        \hline
        0.00329 & 0.00099 & 3.32 &  \url{https://www.salon.com/}$^{1}$  \\
        \hline
        0.00187 & 0.00056 & 3.32 &  \url{https://www.palabranahj.org/archive/}$^{2}$   \\
        \hline
        0.00158 & 0.00047 & 3.32 &   \url{https://www.theatlantic.com/politics/archive/2019/09/}$^{3}$    \\
        \hline
        0.00110 & 0.00033 & 3.32 &   \url{https://www.theatlantic.com/magazine/archive/2020/09/}$^{4}$  \\
        \hline
        0.00104 & 0.00031 & 3.32 &   \url{https://www.theatlantic.com/ideas/archive/2021/01/}$^{5}$   \\
        
        \hline
        0.00096 & 0.00029 & 3.32 &   \url{https://www.theatlantic.com/magazine/archive/2021/03/}$^{6}$   \\
        
        \hline
        0.00090 & 0.00027 & 4.17 &  \url{https://aidnography.blogspot.com/2021/02/}$^{7}$   \\
        \hline
        0.00047 & 0.00014 & 3.32 &   \url{https://web.archive.org/web/20061004124649/}$^{8}$   \\
        \hline
        0.00046 & 0.00014 & 3.32 &  \url{https://harpers.org/archive/2016/04/legalize-it-all/}$^{9}$   \\
        \hline
        0.00016 & 0.00005 & 3.32 &  \url{https://crimethinc.com/2021/03/03/}$^{10}$   \\
        \hline
    \end{tabular}
    \caption{TF-IDF for 10 URIs using my 1000 links for DF}
    \label{tab:my_label}
\end{table}

\subsection*{Discussion}
The IDF was actually lower within my collection of links than that of the one used by Google, but the result was surprisingly close. The difference was less than 1 percent.


%-  *******  Add more *******
%-------------------------------------------------------------------------
\section*{References}
Links used: \\
1. \url{https://www.salon.com/2021/01/20/no-one-should-be-afraid-of-donald-trump-he-is-the-biggest-loser-in-us-political-history/}  \\
2.  \url{https://www.palabranahj.org/archive/the-digital-resistance}  \\
3. \url{https://www.theatlantic.com/politics/archive/2019/09/joe-biden-crime-bill-and-americans-short-memory/597547/}   \\
4. \url{https://www.theatlantic.com/magazine/archive/2020/09/china-ai-surveillance/614197/} \\
5. \url{https://www.theatlantic.com/ideas/archive/2021/01/trump-worst-president-history/617730/}  \\
6. \url{https://www.theatlantic.com/magazine/archive/2021/03/voting-rights-act-democracy/617792/}  \\
7. \url{https://aidnography.blogspot.com/2021/02/development-ict4d-digital-communication-academia-link-review-396.html}  \\
8. \url{https://web.archive.org/web/20061004124649/http://blogs.abcnews.com/theblotter/2006/09/sixteenyearold_.html}  \\
9. \url{https://harpers.org/archive/2016/04/legalize-it-all/}  \\
10. \url{https://crimethinc.com/2021/03/03/the-kronstadt-uprising-a-full-chronology-and-archive-including-a-view-from-within-the-revolt#appendix-a-chronology-of-the-kronstadt-uprising-including-the-text-of-their-daily-paper}  \\
 
\end{document}

